# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jWVgY9DcRdKO2kd1AsEkVshSPuKv93i0
"""

from facenet_pytorch import MTCNN
from tqdm import tqdm
from pathlib import Path
from PIL import Image
import pandas as pd
import subprocess
import torch
import torchvision.transforms.functional as VT
import os

def prepare_ravdess_data(
    data_root: str="data/archive (1)/RAVDESS dataset",
    output_root: str="data/preprocessed_faces",
    fps: int=5,
    face_size: int=224,
    scale_factor: float=1.3,
    device: str='cuda'
) -> pd.DataFrame:
    """
    Prepare RAVDESS dataset by extracting faces and audio from videos.

    Args:
        data_root: Path to RAVDESS dataset root
        output_root: Path to save processed data
        fps: Frames per second to extract
        face_size: Size of face crops
        scale_factor: Scale factor for face margin
        device: Device for MTCNN
    """
    # Initialize MTCNN for face detection
    detector = MTCNN(
        image_size=face_size,
        margin=int(face_size * (scale_factor - 1) / 2),
        device=device,
        select_largest=True,
        post_process=False,
        keep_all=False
    )

    # Create output directories
    output_root = Path(output_root)
    faces_dir = output_root / "faces"
    audio_dir = output_root / "audio"
    temp_frames_dir = output_root / "temp_frames"
    faces_dir.mkdir(parents=True, exist_ok=True)
    audio_dir.mkdir(parents=True, exist_ok=True)
    temp_frames_dir.mkdir(parents=True, exist_ok=True)

    # Define emotion mapping
    emotions = {
        "01": "neutral",
        "02": "calm",
        "03": "happy",
        "04": "sad",
        "05": "angry",
        "06": "fearful",
        "07": "disgust",
        "08": "surprised"
    }

    # Initialize DataFrame
    data = []

    # Process all videos
    video_paths = list(Path(data_root).rglob("*.mp4"))
    for video_path in tqdm(video_paths, desc="Processing videos"):
        try:
            emotion_code = video_path.stem.split('-')[2]
            if emotion_code not in emotions:
                continue

            # Create output directories for this video
            video_id = video_path.stem
            video_faces_dir = faces_dir / video_id
            video_faces_dir.mkdir(exist_ok=True)

            # Create temporary directory for frame extraction
            temp_video_frames = temp_frames_dir / video_id
            temp_video_frames.mkdir(exist_ok=True)

            # Extract audio
            audio_path = audio_dir / f"{video_id}.wav"
            if not os.path.exists(audio_path):
                subprocess.call(
                    ["ffmpeg", "-y", "-i", str(video_path), "-ac", "1", "-ar", "16000", str(audio_path)],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.STDOUT
                )

            # Extract frames using ffmpeg
            subprocess.call(
                ["ffmpeg", "-i", str(video_path),
                 "-vf", f"fps={fps},scale=256:256",
                 str(temp_video_frames / "%03d.jpeg")],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.STDOUT
            )

            # Process extracted frames
            frame_paths = []
            for frame_path in sorted(temp_video_frames.glob("*.jpeg")):
                try:
                    # Read image
                    img = Image.open(frame_path)

                    # Detect and crop face
                    face_img = detector(img)

                    if face_img is not None:
                        output_path = video_faces_dir / f"frame_{len(frame_paths):04d}.png"
                        if isinstance(face_img, torch.Tensor):
                            face_img = VT.ToPILImage()(face_img.to(torch.uint8))
                        face_img.save(output_path)
                        frame_paths.append(output_path)

                except Exception as e:
                    print(f"Error processing frame {frame_path}: {str(e)}")
                    continue

            # Clean up temporary frames
            for frame_path in temp_video_frames.glob("*.jpeg"):
                frame_path.unlink()
            temp_video_frames.rmdir()

            if frame_paths:  # Only add to dataset if faces were detected
                data.append({
                    'video_id': video_id,
                    'frame_dir': video_faces_dir,
                    'audio_path': audio_path,
                    'emotion': emotions[emotion_code],
                    'n_frames': len(frame_paths)
                })

        except Exception as e:
            print(f"Error processing {video_path}: {str(e)}")
            continue

    # Remove temporary directory if empty
    try:
        temp_frames_dir.rmdir()
    except:
        pass

    # Create and save DataFrame
    df = pd.DataFrame(data)
    df['emotion'] = pd.Categorical(df['emotion'])
    df['target'] = df['emotion'].cat.codes
    df.to_csv(output_root / "metadata.csv", index=False)

    return df